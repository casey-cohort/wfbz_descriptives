---
title: "Population Summaries"
toc: true
#jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

Coming soon! 

{{< include _helpers.qmd >}}
            
```{r packages}
library(sf)
library(tidyverse)
library(fs)
library(glue)
library(ipumsr)
library(duckdb)
library(here)
library(arrow)
```

```{r pop_data}
# Download Population data if it doesn't exist

download_spatial_ghs_pop <- function(yr, dst) {
  outfile <- file.path(
    dst,
    glue('GHS_POP_E{yr}_GLOBE_R2023A_54009_100_V1_0.tif')
  )
  if (!fs::file_exists(outfile)) {
    unzip_url(
      glue(
        'https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_POP_GLOBE_R2023A/GHS_POP_E{yr}_GLOBE_R2023A_54009_100/V1-0/GHS_POP_E{yr}_GLOBE_R2023A_54009_100_V1_0.zip'
      ),
      dir_create(dst)
    )
  }
  outfile
}
popfile <- list()
popfile['2000'] <- download_spatial_ghs_pop(2000, here('data/raw/ghs_pop/2000/'))
popfile['2005'] <- download_spatial_ghs_pop(2005, here('data/raw/ghs_pop/2005/'))
popfile['2010'] <- download_spatial_ghs_pop(2010, here('data/raw/ghs_pop/2010/'))
popfile['2015'] <- download_spatial_ghs_pop(2015, here('data/raw/ghs_pop/2015/'))
popfile['2020'] <- download_spatial_ghs_pop(2020, here('data/raw/ghs_pop/2020/'))
```

```{r census_data}
# Pull Census data from NHGIS if it doesn't yet exist
if (!dir_exists(here('data/raw/nhgis'))) {
  dir_create(here('data/raw/nhgis'))
  datasets_to_fetch <- get_metadata_catalog('nhgis', 'data_tables') %>% # find which tables contain variables of interest
    filter(
      name %in%
        c(
          "NP049C",
          "NH030A",
          "NH035A",
          "NH034A",
          "NH044A",
          "B25014",
          "NH069A",
          "NH094A",
          "C24050",
          "B25024",
          "B25035",
          "B25034",
          "B25044",
          "B25070",
          "B25091",
          "B27010"#,
          #"B28011" # internet
        )
    ) %>%
    filter(
      # filter data sets to those from dates of interest
      between(as.integer(substr(dataset_name, 1, 4)), 2000, 2022)
    ) %>%
    nest(.by = dataset_name) %>%
    filter(
      # filter out any data set we found that doesn't have tracts
      unlist(map(
        dataset_name,
        ~ 'tract' %in% get_metadata('nhgis', .x)$geog_levels$name
      ))
    )
  dir_create(here('data/processed/nhgis'), recurse = TRUE)
  write_csv(
    datasets_to_fetch %>% unnest(data),
    here('data/processed/nhgis/nhgis_datasets_pulled.csv')
  )
  dataset_specs <- datasets_to_fetch %>%
    pmap(\(dataset_name, data) {
      ds_spec(dataset_name, data$name, geog_levels = 'tract')
    })

  ts_specs <- map(
    c("B84", "C50", "B79", "C20", "B85", "B37", "A43", "AV0", "CV4"),
    ~ tst_spec(.x, geog_levels = "tract")
  )

  nhgis_extract_request <- define_extract_agg(
    "nhgis",
    description = "WFBZ Population Analysis",
    datasets = dataset_specs,
    time_series_tables = ts_specs,
    tst_layout = "time_by_row_layout"
  )
  submitted_extract <- submit_extract(nhgis_extract_request)
  downloadable_extract <- wait_for_extract(submitted_extract)
  path_to_data_files <- download_extract(
    downloadable_extract,
    download_dir = here('data/raw/nhgis')
  )
  # Get rid of specific data request number and move files out of zip dir
  dir_ls(here('data/raw/nhgis'), glob = '*.zip') %>%
    walk(unzip, exdir = here('data/raw/nhgis'))
  dir_ls(here('data/raw/nhgis'), type = 'directory') %>%
    dir_ls() %>%
    walk(
      ~ file_move(
        .x,
        path(
          here('data/raw/nhgis'),
          str_replace(basename(.x), 'nhgis[0-9]{4}_', '')
        )
      )
    )
  dir_ls(here('data/raw/nhgis'), type = 'directory') %>% walk(fs::dir_delete)
}
```

```{r duckify}
#| cache: True

dir_create(here('data/processed/nhgis/'))
conn <- dbConnect(duckdb(), here('data/processed/nhgis/nhgis.duckdb'))
 for (f in dir_ls(here('data/raw/nhgis'), glob = '*tract.csv')) {
  dbExecute(
    conn,
    glue(
      'CREATE TABLE IF NOT EXISTS {fs::path_ext_remove(basename(f))} AS SELECT * FROM read_csv_auto(\'{f}\', sample_size=-1)'
    )
  )
}
for (f in dir_ls(here('data/raw/nhgis'), glob = '*codebook.txt')) {
  codebook <- read_nhgis_codebook(f)$var_info
  dbWriteTable(
    conn,
    value = codebook,
    name = fs::path_ext_remove(basename(f)),
    overwrite = TRUE
  )
}
```

```{r format_census_data}
#| cache: True
# turn everything into one really long table
tbls <- fs::path_ext_remove(basename(dir_ls(
  here('data/raw/nhgis'),
  glob = '*tract.csv'
)))
long_tbls <- map(
  str_subset(tbls, '^ts', negate = TRUE),
  ~ tbl(conn, .x) %>%
    select(-matches('Margin(s?) of error')) %>%
    pivot_longer(
      -c(YEAR, STATEA, COUNTYA, TRACTA),
      names_to = 'var_name',
      values_drop_na = TRUE
    ) %>%
    inner_join(
      tbl(conn, paste0(.x, '_codebook')) %>% filter(var_desc != '')
    ) %>%
    filter(!grepl(table, 'Margin(s?) of error')) %>%
    mutate(GEOID = paste0(STATEA, COUNTYA, TRACTA)) %>%
    select(
      GEOID,
      YEAR,
      code = var_name,
      table = var_desc,
      category = var_label,
      value
    )
) %>%
  append(
    map(
      str_subset(tbls, '^ts'),
      ~ tbl(conn, .x) %>%
        pivot_longer(
          -c(YEAR, STATEFP, COUNTYFP, TRACTA),
          names_to = 'var_name',
          values_drop_na = TRUE
        ) %>%
        inner_join(
          tbl(conn, paste0(.x, '_codebook')) %>% filter(var_desc != '')
        ) %>%
        filter(!grepl(table, 'Margin(s?) of error')) %>%
        mutate(GEOID = paste0(STATEFP, COUNTYFP, TRACTA)) %>%
        select(
          GEOID,
          YEAR,
          code = var_name,
          table = var_desc,
          category = var_label,
          value
        )
    )
  ) %>%
  reduce(union_all) %>%
  compute(name = paste0('all_long'), overwrite = TRUE, temporary = FALSE)

```

```{r}
# Pull all data, categorize, name, and break it up into our output vars of interst
var_types_ts <- tribble(
  ~nhgis_code , ~var_type        ,
  'AV0'       , 'population'     ,
  'B84'       , 'unemployment'   ,
  'C50'       , 'commute_time'   ,
  'B79'       , 'income'         ,
  'C20'       , 'poverty_lt_100' ,
  'B37'       , 'renters'        ,
  'A43'       , 'housing_vacant' ,
  'CV4'       , 'race'           ,
  'B85'       , 'education'      ,
)
var_types_ds <- tribble(
  ~name    , ~var_type             ,
  'NP049C' , 'industry'            ,
  'C24050' , 'industry'            ,
  'B27010' , 'health_insurance'    ,
  'NH030A' , 'housing_type'        ,
  'B25024' , 'housing_type'        ,
  'NH035A' , 'housing_year'        ,
  'B25035' , 'housing_year'        ,
  'NH034A' , 'housing_year'        ,
  'B25034' , 'housing_year'        ,
  'NH044A' , 'vehicle_avail'       ,
  'B25044' , 'vehicle_avail'       ,
  'B28011' , 'internet'            ,
  'B25014' , 'housing_crowding'    ,
  'NH069A' , 'housing_cost_burden' ,
  'NH094A' , 'housing_cost_burden' ,
  'B25070' , 'housing_cost_burden' ,
  'B25091' , 'housing_cost_burden' ,
) %>%
  left_join(get_metadata_catalog('nhgis', 'data_tables')) %>%
  select(nhgis_code, var_type) %>%
  distinct()


```

```{r}
all_long <- 
  tbl(conn, 'all_long') %>%
  mutate(nhgis_code = regexp_replace(regexp_extract(table, 'Table [A-Z0-9]+'), 'Table ', '')) %>%
  left_join(bind_rows(var_types_ds, var_types_ts), copy = TRUE) %>%
  select(GEOID, table, YEAR, var_type, category, value)

#walk(
#  all_long %>% select(var_type) %>% distinct() %>% pull(), # for each var_type
#  function(.x){
#    all_long %>% 
#      filter(var_type == .x) %>%
#      pivot_wider(id_cols = c(GEOID, table, YEAR, var_type), values_from = value, names_from = category) %>%
#      compute(name = .x, overwrite = TRUE, temporary = FALSE)
#  }
#)

all_long_processed <- list()
all_long_processed$commute_time <- all_long %>%
  filter(var_type == 'commute_time') %>% 
  group_by(GEOID, YEAR) %>%
  filter(!str_detect(category, 'Margin of error')) %>%
  transmute(
    GEOID,
    category,
    period = YEAR,
    start_year = as.integer(regexp_extract(YEAR, '^\\d{4}')),
    end_year = as.integer(regexp_extract(YEAR, '\\d{4}$')),
    commute_time_min = case_when(
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ Less than 5 minutes" ~ 0,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 5 to 9 minutes" ~ 5,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 10 to 14 minutes" ~ 10,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 15 to 19 minutes" ~ 15,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 20 to 29 minutes" ~ 20,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 30 to 44 minutes" ~ 30,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 45 to 59 minutes" ~ 45,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 60 or more minutes" ~ 60
    ),
    commute_time_max = case_when(
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ Less than 5 minutes" ~ 4,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 5 to 9 minutes" ~ 9,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 10 to 14 minutes" ~ 14,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 15 to 19 minutes" ~ 19,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 20 to 29 minutes" ~ 29,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 30 to 44 minutes" ~ 44,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 45 to 59 minutes" ~ 59,
      category ==
        "Persons: Worker ~ 16 years and over ~ Did not work from home ~ 60 or more minutes" ~ 999
    ),
    count = as.numeric(value),
    frac = count / sum(count, na.rm = TRUE)
  ) %>%
  collect() 

all_long_processed$education <- all_long %>%
  filter(var_type == 'education') %>%
  group_by(GEOID, YEAR) %>%
  filter(!str_detect(category, 'Margin of error')) %>%
  transmute(
    GEOID,
    category,
    period = YEAR,
    start_year = as.integer(regexp_extract(YEAR, '^\\d{4}')),
    end_year = as.integer(regexp_extract(YEAR, '\\d{4}$')),
    education = str_replace(category, 'Persons: 25 years and over ~ ', ''),
    education_order = case_when(
      str_detect(category, "Less than 9th grade") ~ 1,
      str_detect(category, "9th to 12th grade, no diploma") ~ 2,
      str_detect(category, "High school graduate, GED, or alternative") ~ 3,
      str_detect(category, "Some college, no degree") ~ 4,
      str_detect(category, "Associate's degree") ~ 5,
      str_detect(category, "Bachelor's degree") ~ 6,
      str_detect(category, "Graduate or professional degree") ~ 7
    ),
    count = as.numeric(value),
    frac = count / sum(count, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  collect() 

all_long_processed$housing_cost_burden <- all_long %>%
  filter(var_type == 'housing_cost_burden') %>% 
  filter(!str_detect(category, 'Margin of error')) %>%
  group_by(GEOID, YEAR) %>%
  filter(
    (str_detect(
      table,
      "Mortgage Status by Selected Monthly Owner Costs as a Percentage of Household Income in the Past 12 Months"
    ) &
      str_detect(category, "^Estimates")) |
      (str_detect(
        table,
        "Gross Rent as a Percentage of Household Income in the Past 12 Months"
      ) &
        str_detect(category, "^Estimates"))
  ) %>%
  mutate(
    GEOID,
    category,
    period = YEAR,
    start_year = as.integer(regexp_extract(YEAR, '^\\d{4}')),
    end_year = as.integer(regexp_extract(YEAR, '\\d{4}$')),
    education = str_replace(category, 'Persons: 25 years and over ~ ', ''),
    cost_burdened = (str_detect(
      table,
      "Mortgage Status by Selected Monthly Owner Costs as a Percentage of Household Income in the Past 12 Months"
    ) &
      str_detect(category, "50.0 percent or more")) |
      (str_detect(
        table,
        "Gross Rent as a Percentage of Household Income in the Past 12 Months"
      ) &
        str_detect(category, "50.0 percent or more")),
    count = as.numeric(value)
  ) %>%
  group_by(GEOID, category, period, start_year, end_year, cost_burdened) %>%
  summarize(
    count = sum(as.numeric(value), na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  group_by(GEOID, category, period, start_year, end_year) %>%
  mutate(
    frac = count / sum(count),
  ) %>%
  ungroup() %>%
  collect() 

all_long_processed$poverty_lt_100 <-
  all_long %>%
  filter(var_type == 'poverty_lt_100') %>% 
  group_by(GEOID, YEAR) %>%
  filter(!str_detect(category, 'Margin of error')) %>%
  mutate(
    period = YEAR,
    start_year = as.integer(regexp_extract(YEAR, '^\\d{4}')),
    end_year = as.integer(regexp_extract(YEAR, '\\d{4}$')),
    poverty_lt_100 = coalesce(
      sql("try_cast(regexp_extract(category, '[0-9\\.]+$') as integer)"),
      999
    ) <
      1.0,
  ) %>%
  group_by(GEOID, category, period, start_year, end_year, poverty_lt_100) %>%
  summarize(
    count = sum(as.numeric(value), na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  group_by(GEOID, category, period, start_year, end_year) %>%
  mutate(
    frac = count / sum(count),
  ) %>%
  ungroup() %>%
  collect() 


all_long_processed$vehicle_avail <-
  all_long %>%
  filter(var_type == 'vehicle_avail') %>% 
  group_by(GEOID, YEAR) %>%
  filter(str_detect(category, 'Estimates'), !str_detect(category, "Total")) %>%
  mutate(
    period = YEAR,
    start_year = as.integer(regexp_extract(YEAR, '^\\d{4}')),
    end_year = as.integer(regexp_extract(YEAR, '\\d{4}$')),
    no_vehicle = str_detect(category, "No vehicle"),
  ) %>%
  group_by(GEOID, category, period, start_year, end_year, no_vehicle) %>%
  summarize(
    count = sum(as.numeric(value), na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  group_by(GEOID, category, period, start_year, end_year) %>%
  mutate(
    frac = count / sum(count),
  ) %>%
  ungroup() %>%
  collect() 

all_long_processed$race <- all_long %>%
  filter(var_type == 'race') %>%
  filter(!str_detect(category, 'Margin of error')) %>% 
  group_by(GEOID, YEAR) %>%
  transmute(
    GEOID,
    category, 
    period = YEAR,
    start_year = as.integer(regexp_extract(YEAR, '^\\d{4}')),
    end_year = as.integer(regexp_extract(YEAR, '\\d{4}$')),
    race_ethnicity = category,
    ethnicity = regexp_extract(category, '(Not )?Hispanic or Latino'),
    race = regexp_replace(regexp_extract(category, '~ .*$'), '^~ ', ''),
    count = as.numeric(value),
    frac = count / sum(count, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  collect() 

#all_long_processed <- map(all_long_processed, ~mutate(.x, category = as.factor(category)))

walk(
  names(all_long_processed), 
  ~write_parquet(all_long_processed[[.x]], here('data/processed/nhgis/', paste0(.x, '.parquet')))
)

```

## Maps!

ACS 2015-2019, Chittenden County all_long


Housing Cost Burden (>50%)

```{r}

library(tigris)
library(sf)
tracts_sf <- tracts(
  state = 'vt',
  county = 'Chittenden',
  cb = TRUE,
  year = 2010
) %>%
  mutate(GEOID = paste0(STATE, COUNTY, TRACT)) %>%
  tigris::erase_water(area_threshold = .9)

inner_join(
  tracts_sf,
  all_long_processed$housing_cost_burden %>%
    filter(year == '2015-2019', cost_burdened),
  by = "GEOID"
) %>%
  ggplot() +
  geom_sf(
    color = NA,
    aes(fill = frac)
  ) +
  scale_fill_distiller(palette = 'Spectral') +
  theme_void()

```


Poverty (>100% Poverty Level)


```{r}

inner_join(
  tracts_sf,
  all_long_processed$poverty_lt_100 %>% filter(year == '2015-2019', poverty_lt_100),
  by = "GEOID"
) %>%
  ggplot() +
  geom_sf(
    color = NA,
    aes(fill = frac)
  ) +
  scale_fill_distiller(palette = 'Spectral', name = '') +
  theme_void() +
  ggtitle("Population Above 100% Poverty Limit")

```

Vehicle Available

```{r}

inner_join(
  tracts_sf,
  all_long_processed$vehicle_avail %>% filter(year == '2015-2019', !no_vehicle),
  by = "GEOID"
) %>%
  ggplot() +
  geom_sf(
    color = NA,
    aes(fill = frac)
  ) +
  scale_fill_distiller(palette = 'Spectral', name = '') +
  theme_void() +
  ggtitle("Vehicle Available in Household")
```

Race (% NH White)

```{r}

inner_join(
  tracts_sf,
  all_long_processed$race %>%
    filter(
      year == '2015-2019',
      str_detect(race, 'White'),
      str_detect(ethnicity, 'Not')
    ),
  by = "GEOID"
) %>%
  ggplot() +
  geom_sf(
    color = NA,
    aes(fill = frac)
  ) +
  scale_fill_distiller(palette = 'Spectral', name = '') +
  theme_void() +
  ggtitle("Non-Hispanic White")
```
